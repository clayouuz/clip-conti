{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomTextCLIP(\n",
       "  (visual): TimmModel(\n",
       "    (trunk): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn_pool): AttentionPoolLatent(\n",
       "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (kv): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (head): Sequential()\n",
       "  )\n",
       "  (text): TextTransformer(\n",
       "    (token_embedding): Embedding(32000, 768)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (text_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model and tokenizer\n",
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "# model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "# tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:timm/ViT-B-16-SigLIP-384')\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP-384')\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open_clip.model.CustomTextCLIP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.datasets import CIFAR100\n",
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root='../data', download=False, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化示例（不使用text模型进行预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pic_to_tensor(image=None,image_path=None):\n",
    "    if image_path is not None:\n",
    "        image = Image.open(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(\"Either image or image_path should be provided\")\n",
    "    inputs = preprocess_val(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        inputs = model.visual(inputs).float()\n",
    "    return inputs\n",
    "pic_to_tensor(cifar100[42][0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cifar100上的示例,使用了text model进行zero shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = torch.cat([tokenizer(f\"{c}\") for c in cifar100.classes])\n",
    "text_features = model.encode_text(text_inputs)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "# import pickle\n",
    "# with open('text_features.pkl', 'wb') as f:\n",
    "#     pickle.dump(text_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label: mountain\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 predictions:\n",
      "\n",
      "        mountain: 78.01%\n",
      "           plain: 5.39%\n",
      "             sea: 2.04%\n",
      "           cloud: 1.63%\n",
      "            road: 0.87%\n",
      "            seal: 0.78%\n",
      "          forest: 0.65%\n",
      "           plate: 0.50%\n",
      "            bear: 0.49%\n",
      "            bowl: 0.45%\n",
      "           mouse: 0.38%\n",
      "             can: 0.37%\n",
      "          rocket: 0.33%\n",
      "             cup: 0.31%\n",
      "          bridge: 0.30%\n",
      "            rose: 0.27%\n",
      "       butterfly: 0.26%\n",
      "           train: 0.25%\n",
      "        mushroom: 0.24%\n",
      "           apple: 0.24%\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def testImage(image,label=None,show=False,type=\"image\"):\n",
    "    if type==\"image\":\n",
    "        image_input = preprocess_val(image).unsqueeze(0)\n",
    "        image_features = model.encode_image(image_input)\n",
    "    elif type==\"tensor\":\n",
    "        image_features = image\n",
    "    \n",
    "    # Pick the top 5 most similar labels for the image\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    # text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity[0].topk(20)\n",
    "    if show:\n",
    "        print(\"true label:\",label)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')  # 关闭坐标轴\n",
    "        plt.show()\n",
    "        \n",
    "        # Print the result\n",
    "        print(\"\\nTop 20 predictions:\\n\")\n",
    "        for value, index in zip(values, indices):\n",
    "            print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "    return indices[0].item()\n",
    "        \n",
    "print(testImage(cifar100[42][0],cifar100.classes[cifar100[42][1]],show=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分为训练集和测试集\n",
    "from tqdm import tqdm\n",
    "tensor_dataset=[]\n",
    "for item in tqdm(cifar100):\n",
    "    tensor_dataset.append([pic_to_tensor(item[0]),item[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset , test_dataset = train_test_split(cifar100, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "for i in tensor_dataset:\n",
    "    i[0]=i[0].cpu()\n",
    "# # 保存到文件\n",
    "with open('tensor_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(tensor_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从文件加载变量\n",
    "import pickle\n",
    "with open('tensorbook.pkl', 'rb') as f:\n",
    "    tensor_dataset = pickle.load(f)\n",
    "train_size = int(0.8 * len(tensor_dataset))\n",
    "test_size = len(tensor_dataset) - train_size\n",
    "\n",
    "# with open('text_features.pkl', 'rb') as f:\n",
    "#     text_features = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.242137e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "old_text_features=old_text_features.cpu().detach().numpy()\n",
    "text_features=text_features.cpu().detach().numpy()\n",
    "old_new_distance=np.linalg.norm(old_text_features-text_features)\n",
    "#print the distance between the same image\n",
    "print(old_new_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5762787e-07"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(old_text_features-text_features).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 8434.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#直接使用tensor进行测试\n",
    "import tqdm\n",
    "import torch\n",
    "acc=0\n",
    "\n",
    "def testImage(image,label=None,show=False,type=\"image\"):\n",
    "    if type==\"tensor\":\n",
    "        image_features = image\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity[0].topk(20)\n",
    "    return indices[0].item()\n",
    "\n",
    "for i in tqdm.tqdm(test_dataset):\n",
    "    temp=testImage(i[0],cifar100.classes[i[1]],show=False,type=\"tensor\")\n",
    "    if temp==i[1]:\n",
    "        acc+=1\n",
    "print(acc/len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率为：0.7138\n"
     ]
    }
   ],
   "source": [
    "#knn\n",
    "import torch\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 提取训练集特征和标签\n",
    "train_features = [data[0].numpy().flatten() for data in train_dataset]\n",
    "train_labels = [data[1] for data in train_dataset]\n",
    "\n",
    "# 创建 KNN 分类器\n",
    "knn = KNeighborsClassifier(n_neighbors=10,weights='distance')\n",
    "\n",
    "# 训练分类器\n",
    "knn.fit(train_features, train_labels)\n",
    "\n",
    "# 提取测试集特征\n",
    "\n",
    "test_features = [data[0].numpy().flatten() for data in test_dataset]\n",
    "test_labels = [data[1] for data in test_dataset]\n",
    "\n",
    "# 进行预测\n",
    "predictions = knn.predict(test_features)\n",
    "\n",
    "correct_predictions = 0\n",
    "for true_label, predicted_label in zip(test_labels, predictions):\n",
    "    if true_label == predicted_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / len(test_labels)\n",
    "print(f\"准确率为：{accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
